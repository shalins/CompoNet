{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68088287",
   "metadata": {},
   "source": [
    "# Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57cae9",
   "metadata": {},
   "source": [
    "This python notebook consists of a set of functions that take a JSON input from the `scraper` tool, and get the data into a format that we can serve on the final website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02295a5d",
   "metadata": {},
   "source": [
    "## JSON to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9d18f",
   "metadata": {},
   "source": [
    "As our input data is currently formatted in JSON, we want to convert that to CSV so we can work with entire columns, and eventually push to a database like PostgreSQL.\n",
    "\n",
    "To avoid corrupting the original data that was collected, place a copy of the data in the convenience folder `./_raw_json` in this directory, and only modify this copied data as you work with the modules in this notebook.\n",
    "\n",
    "The following block of code will go through all the JSON files in the specified directory, flatten them, convert them into CSV's, and save them all to disk in the `./_raw_csv` directory.\n",
    "\n",
    "**NOTE**: This is the only part of the process that's somewhat hardcoded. In order to combine the JSON into a single CSV, the column names have to be consistently named, and its easier to easier to acheive this by edit the JSON's before saving as CSV's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e43e7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from flatten_json import flatten\n",
    "from tqdm import tqdm\n",
    "\n",
    "JSON_DIR = './_raw_json'\n",
    "CSV_DIR = './_raw_csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5741930d",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e659ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_keys_from_dict(d, to_delete):\n",
    "    if isinstance(to_delete, str):\n",
    "        to_delete = [to_delete]\n",
    "    if isinstance(d, dict):\n",
    "        for single_to_delete in set(to_delete):\n",
    "            if single_to_delete in d:\n",
    "                del d[single_to_delete]\n",
    "        for k, v in d.items():\n",
    "            delete_keys_from_dict(v, to_delete)\n",
    "    elif isinstance(d, list):\n",
    "        for i in d:\n",
    "            delete_keys_from_dict(i, to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb28ce",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f781895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:12<00:00, 18.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(os.listdir(JSON_DIR)):\n",
    "    if \".json\" in filename:\n",
    "        with open(f'{JSON_DIR}/{filename}', 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # 1. First globally delete all the keys that we don't want.\n",
    "            delete_keys_from_dict(data, ['__typename', 'best_datasheet', 'best_image', 'manufacturer_url'])\n",
    "            \n",
    "            for result in data['data']['search']['results']:\n",
    "                # 2. Within the JSON, flatten the `specs` array from \n",
    "                #    'specs': [{'attribute': {'id': '548',\n",
    "                #                             'name': 'Capacitance' \n",
    "                #                             'shortname': 'capacitance'\n",
    "                #                             '__typename': 'Attribute'\n",
    "                #                            },\n",
    "                #               'display_value': '100 nF'\n",
    "                #              },\n",
    "                #              { ... },\n",
    "                #              { ... },\n",
    "                #              ...\n",
    "                #             ]\n",
    "                #    to\n",
    "                #    'specs': {'capacitance': {'display_value': '100 nF', 'id': '548'},\n",
    "                #              'case_package': {'display_value': 'Radial', 'id': '842'},\n",
    "                #              'depth': {'display_value': '8 mm', 'id': '291'},\n",
    "                #              ...\n",
    "                #             }    \n",
    "                #    and remove some fields that we don't want to include.\n",
    "                spec_json = {}\n",
    "                for spec in result['part']['specs']:\n",
    "                    title = spec['attribute']['shortname']\n",
    "                    spec['attribute']['display_value'] = spec['display_value']\n",
    "                    spec = spec['attribute']\n",
    "                    del spec['shortname']\n",
    "                    del spec['name']\n",
    "                    spec_json[title] = spec\n",
    "                result['part']['specs'] = spec_json\n",
    "\n",
    "\n",
    "                # 3. Remove specific parts of the JSON that we don't want (duplicate fields, etc).\n",
    "                del result['part']['_cache_id']\n",
    "                del result['part']['descriptions']\n",
    "                del result['part']['counts']\n",
    "            \n",
    "            # 4. Run the `flatten` function on each of the parts, place it in a list, and convert \n",
    "            #    to a Pandas DF.\n",
    "            flat = [flatten(d) for d in data['data']['search']['results']]\n",
    "    \n",
    "            df = pd.DataFrame(flat, dtype ='str')\n",
    "            df.to_csv(f\"{CSV_DIR}/{filename.split('.')[0]}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885f0d4",
   "metadata": {},
   "source": [
    "#### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db05f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./_raw_csv/film.csv', './_raw_csv/mica.csv', './_raw_csv/ceramic.csv', './_raw_csv/aluminum_electrolytic.csv']\n"
     ]
    }
   ],
   "source": [
    "filenames = [f\"{CSV_DIR}/{filename}\" for filename in os.listdir(CSV_DIR) if \"all\" not in filename and \".csv\" in filename]\n",
    "df = pd.concat(map(pd.read_csv, filenames), ignore_index=True)\n",
    "df = df.astype(str)\n",
    "df.to_csv(f\"{CSV_DIR}/all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4661d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
