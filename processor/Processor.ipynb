{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7074b8d9",
   "metadata": {},
   "source": [
    "# Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603377b",
   "metadata": {},
   "source": [
    "This python notebook consists of a set of functions that take a JSON input from the `scraper` tool, and get the data into a format that we can serve on the final website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169e6fa",
   "metadata": {},
   "source": [
    "## JSON to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86d2c9",
   "metadata": {},
   "source": [
    "As our input data is currently formatted in JSON, we want to convert that to CSV so we can work with entire columns, and eventually push to a database like PostgreSQL.\n",
    "\n",
    "To avoid corrupting the original data that was collected, place a copy of the data in the convenience folder `./_raw_json/` in this directory, and only modify this copied data as you work with the modules in this notebook.\n",
    "\n",
    "For the script to work properly, the `./_raw_json/` directory _must_ be partitioned into subdirectories of `year`, and json's must be nested inside each of the corresponding years. \n",
    "\n",
    "A correct directory structure would look something like this:\n",
    "```\n",
    "_raw_json\n",
    "    2022\n",
    "        aluminum_electrolytic.json\n",
    "        ceramic.json\n",
    "        film.json\n",
    "        mica.json\n",
    "        polymer.json\n",
    "        tantalum.json\n",
    "    2023\n",
    "        aluminum_electrolytic.json\n",
    "        capacitor_arrays.json\n",
    "        capacitor_kits.json\n",
    "        ceramic.json\n",
    "        film.json\n",
    "        mica.json\n",
    "        polymer.json\n",
    "        tantalum.json\n",
    "```\n",
    "\n",
    "The following block of code will go through all the JSON files in the specified directory, flatten them, convert them into CSV's, and save them all to disk in the `./_raw_csv/` directory.\n",
    "\n",
    "**NOTE**: This is the only part of the process that's somewhat hardcoded. In order to combine the JSON into a single CSV, the column names have to be consistently named, and its easier to acheive this by editing the JSON's before saving as CSV's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2d10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%aimport compute\n",
    "%aimport categories\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from flatten_json import flatten\n",
    "from tqdm import tqdm\n",
    "\n",
    "JSON_DIR = './_raw_json'\n",
    "CSV_DIR = './_raw_csv'\n",
    "CSV_COMBINED = 'combined.csv'\n",
    "CSV_FINAL = 'final.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cf6da",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343f1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_keys_from_dict(d, to_delete):\n",
    "    if isinstance(to_delete, str):\n",
    "        to_delete = [to_delete]\n",
    "    if isinstance(d, dict):\n",
    "        for single_to_delete in set(to_delete):\n",
    "            if single_to_delete in d:\n",
    "                del d[single_to_delete]\n",
    "        for k, v in d.items():\n",
    "            delete_keys_from_dict(v, to_delete)\n",
    "    elif isinstance(d, list):\n",
    "        for i in d:\n",
    "            delete_keys_from_dict(i, to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8125279",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3227a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26886.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:59<00:00,  8.45s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 10/10 [03:14<00:00, 19.47s/it]\n"
     ]
    }
   ],
   "source": [
    "for subdir, dirs, files in os.walk(JSON_DIR):\n",
    "    for filename in tqdm(files):\n",
    "        if \".json\" in filename:\n",
    "            year = subdir.split(\"/\")[-1]\n",
    "            with open(os.path.join(subdir, filename), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                # 1. First globally delete all the keys that we don't want.\n",
    "                delete_keys_from_dict(data, ['__typename', 'best_datasheet', 'best_image', 'manufacturer_url'])\n",
    "\n",
    "                for result in data['data']['search']['results']:\n",
    "                    # 2. Within the JSON, flatten the `specs` array from \n",
    "                    #    'specs': [{'attribute': {'id': '548',\n",
    "                    #                             'name': 'Capacitance' \n",
    "                    #                             'shortname': 'capacitance'\n",
    "                    #                             '__typename': 'Attribute'\n",
    "                    #                            },\n",
    "                    #               'display_value': '100 nF'\n",
    "                    #              },\n",
    "                    #              { ... },\n",
    "                    #              { ... },\n",
    "                    #              ...\n",
    "                    #             ]\n",
    "                    #    to\n",
    "                    #    'specs': {'capacitance': {'display_value': '100 nF', 'id': '548'},\n",
    "                    #              'case_package': {'display_value': 'Radial', 'id': '842'},\n",
    "                    #              'depth': {'display_value': '8 mm', 'id': '291'},\n",
    "                    #              ...\n",
    "                    #             }    \n",
    "                    #    and remove some fields that we don't want to include.\n",
    "                    spec_json = {}\n",
    "                    for spec in result['part']['specs']:\n",
    "                        title = spec['attribute']['shortname']\n",
    "                        spec['attribute']['display_value'] = spec['display_value']\n",
    "                        spec = spec['attribute']\n",
    "                        del spec['shortname']\n",
    "                        del spec['name']\n",
    "                        spec_json[title] = spec\n",
    "                    result['part']['specs'] = spec_json\n",
    "\n",
    "                    # 3. Remove specific parts of the JSON that we don't want (duplicate fields, etc).\n",
    "                    del result['part']['_cache_id']\n",
    "                    del result['part']['descriptions']\n",
    "                    del result['part']['counts']\n",
    "                    \n",
    "                    # 4. Add column for the year of the component based on the subdirectory name.\n",
    "                    result[year] = True\n",
    "\n",
    "                # 5. Run the `flatten` function on each of the parts, place it in a list, and convert \n",
    "                #    to a Pandas DF.\n",
    "                flat = [flatten(d) for d in data['data']['search']['results']]\n",
    "\n",
    "                df = pd.DataFrame(flat, dtype ='str')\n",
    "                if not os.path.exists(f\"{CSV_DIR}/{year}\"):\n",
    "                    os.makedirs(f\"{CSV_DIR}/{year}\")\n",
    "                df.to_csv(f\"{CSV_DIR}/{year}/{filename.split('.')[0]}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b071302",
   "metadata": {},
   "source": [
    "#### Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c4fe2",
   "metadata": {},
   "source": [
    "The combine block aims to:\n",
    "1. Create a list of years based on the file structure of the `./_raw_csv/` directory.\n",
    "2. Loop through each file, and add a `year` column to each of the CSV's. For each column that changes year over year, we create a new column, appending the year at the end. E.g. `original_name_year`.\n",
    "3. Merge CSV's based on 'part_mpn', resolving conflicts using the most recent year.\n",
    "4. Drops the intermediate columns including the `year` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b69032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/6 [00:00<?, ?it/s]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (81,93,97,99,102,106,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      " 17%|██████████████▌                                                                        | 1/6 [00:00<00:01,  2.58it/s]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (83,89,91,100,104,108,110,116,118,120,122,124,126,128,130,132,134,136,142,144,146,148,150,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      " 33%|█████████████████████████████                                                          | 2/6 [00:01<00:02,  1.78it/s]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (14,20,22,24,30,32,36,38,40,46,50,52,54,62,64,66,68,74,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,122,130,132,134,136,140,142,144,146,148,150,152,154,156,158,160,162,164,168,170,172,174,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214,216,218,220,222,224,226,228,230,232,234,236,238,240,242,244,246,248,252,254) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      " 67%|██████████████████████████████████████████████████████████                             | 4/6 [00:04<00:02,  1.14s/it]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (102,106,108,112,118,120,124,128,130,132,134,136,138,140,142,144,146,148,150,152) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.24it/s]\n",
      "  0%|                                                                                               | 0/9 [00:00<?, ?it/s]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (97,99,101,103,110,112,116,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      " 11%|█████████▋                                                                             | 1/9 [00:00<00:06,  1.28it/s]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (83,92,94,96,106,116,118,120,124,126,128,142) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      " 56%|████████████████████████████████████████████████▎                                      | 5/9 [00:00<00:00,  6.82it/s]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (93,97,99,108,110,112,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      "/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (79,93,95,97,110,112,116,118,122,124,126,128,130,132,134,136,138,140,142,144,148,150,156,158,160,162,164,166,168,170,172,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      " 78%|███████████████████████████████████████████████████████████████████▋                   | 7/9 [00:02<00:00,  2.94it/s]/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3705789061.py:8: DtypeWarning: Columns (14,20,28,34,38,52,56,58,60,66,70,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,143,146,150,152,154,158,160,162,164,166,168,170,172,174,176,178,180,182,186,188,190,192,194,196,198,200,204,206,208,210,212,214,216,218,220,222,224,226,228,230,232,236,238,240,242,244,246,248,250,252,254,256,258,262,266,268,270,272,274,276,278,280,282,284,286,288) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(year_dir, filename))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:13<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "years = [d for d in os.listdir(CSV_DIR) if os.path.isdir(os.path.join(CSV_DIR, d))]\n",
    "\n",
    "dataframes = []\n",
    "for year in years:\n",
    "    year_dir = os.path.join(CSV_DIR, year)\n",
    "    for filename in tqdm(os.listdir(year_dir)):\n",
    "        if \"all\" not in filename and filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(year_dir, filename))\n",
    "            df = compute.add_year(df, year)\n",
    "            df = compute.rename_by_year(df, categories.columns_that_update_yearly_preprocess.keys(), year)\n",
    "            dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "combined_df = compute.merge(combined_df, years)\n",
    "combined_df = compute.drop_columns(combined_df, ['year'])\n",
    "combined_df.to_csv(os.path.join(CSV_DIR, CSV_COMBINED), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ce95f",
   "metadata": {},
   "source": [
    "## CSV Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f3bca",
   "metadata": {},
   "source": [
    "In this step, we want to take the combined CSV that we generated in the previous step and format it into the final format that we will upload to the PostgreSQL database.\n",
    "\n",
    "We will use a modular approach. For each step of updating the CSV, we will implement a function that takes in a pandas dataframe and outputs another pandas dataframe in the desired format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1ac143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/pp5p0yss3xq_1z5gvfm0shgh0000gn/T/ipykernel_87486/3219921549.py:1: DtypeWarning: Columns (14,20,22,24,26,28,30,32,34,36,40,46,50,52,54,56,60,62,66,68,72,73,75,77,79,81,85,87,89,91,93,95,97,99,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,158,160,162,164,166,168,170,172,178,180,182,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214,216,218,220,222,224,226,228,230,232,234,236,242,244,246,248,250,252,254,258,260,262,266,268,270,272,274,276,278,280,282,284,286,288,290,292,294,296,298,300,302,304,306,310,312,314,316,318,320,322,324,325,327,329,331,333,334,340,342,344,346,348,350,352,354,356,358,360,362,364) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'{CSV_DIR}/{CSV_COMBINED}', index_col=False)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{CSV_DIR}/{CSV_COMBINED}', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be588ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [d for d in os.listdir(CSV_DIR) if os.path.isdir(os.path.join(CSV_DIR, d))]\n",
    "\n",
    "year_specific_columns = {}\n",
    "year_specific_columns_postprocessed = []\n",
    "for year in years:\n",
    "    for col, data_type in categories.columns_that_update_yearly_preprocess.items():\n",
    "        year_specific_col = f\"{col}_{year}\"\n",
    "        if data_type not in year_specific_columns:\n",
    "            year_specific_columns[data_type] = []\n",
    "        year_specific_columns[data_type].append(year_specific_col)\n",
    "    for col in categories.columns_that_update_yearly_postprocess:\n",
    "        year_specific_columns_postprocessed.append(f\"{col}_{year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5292297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_float_cols = [\n",
    "    'part_specs_tolerance_display_value', \n",
    "    'part_specs_temperaturecoefficient_display_value', \n",
    "    'part_specs_maxjunctiontemperature_display_value', \n",
    "    'part_specs_maxoperatingtemperature_display_value', \n",
    "    'part_specs_minoperatingtemperature_display_value', \n",
    "    'part_specs_dissipationfactor_display_value', \n",
    "    'part_specs_failurerate_display_value', \n",
    "    'part_specs_frequencytolerance_display_value', \n",
    "    'part_specs_qfactor_display_value', \n",
    "    'part_specs_frequencystability_display_value', \n",
    "    'part_specs_accuracy_display_value', \n",
    "    'part_specs_speedgrade_display_value', \n",
    "    'part_specs_inductancetolerance_display_value', \n",
    "    'part_specs_ambienttemperaturerangehigh_display_value'\n",
    "]\n",
    "\n",
    "string_to_int_cols = [\n",
    "    'part_specs_numberofpins_display_value', \n",
    "    'part_specs_life_hours__display_value', \n",
    "    'part_specs_life_cycles__display_value', \n",
    "    'part_specs_numberofcapacitors_display_value', \n",
    "]\n",
    "\n",
    "string_to_base_float_cols = [\n",
    "    'part_specs_capacitance_display_value', \n",
    "    'part_specs_depth_display_value', \n",
    "    'part_specs_height_display_value', \n",
    "    'part_specs_height_seated_max__display_value', \n",
    "    'part_specs_length_display_value', \n",
    "    'part_specs_voltage_display_value', \n",
    "    'part_specs_voltagerating_display_value', \n",
    "    'part_specs_voltagerating_ac__display_value', \n",
    "    'part_specs_voltagerating_dc__display_value', \n",
    "    'part_specs_width_display_value', \n",
    "    'part_specs_weight_display_value', \n",
    "    'part_specs_insulationresistance_display_value', \n",
    "    'part_specs_diameter_display_value', \n",
    "    'part_specs_thickness_display_value', \n",
    "    'part_specs_esr_equivalentseriesresistance__display_value', \n",
    "    'part_specs_resistance_display_value', \n",
    "    'part_specs_dcresistance_dcr__display_value', \n",
    "    'part_specs_inductance_display_value', \n",
    "    'part_specs_maxdccurrent_display_value', \n",
    "    'part_specs_powerrating_display_value', \n",
    "    'part_specs_seriesresistance_display_value', \n",
    "    'part_specs_currentrating_display_value', \n",
    "    'part_specs_characterheight_display_value', \n",
    "    'part_specs_ripplecurrent_display_value',\n",
    "    'part_specs_maxlength_display_value', \n",
    "    'part_specs_maxthickness_display_value', \n",
    "    'part_specs_maxwidth_display_value', \n",
    "    'part_specs_minlength_display_value', \n",
    "    'part_specs_minthickness_display_value', \n",
    "    'part_specs_minwidth_display_value', \n",
    "    'part_specs_insidediameter_display_value', \n",
    "    'part_specs_selfresonantfrequency_display_value', \n",
    "    'part_specs_current_display_value', \n",
    "    'part_specs_maxcurrentrating_display_value', \n",
    "    'part_specs_maxvoltagerating_dc__display_value', \n",
    "    'part_specs_maxfrequency_display_value', \n",
    "    'part_specs_leakagecurrent_display_value', \n",
    "    'part_specs_testfrequency_display_value', \n",
    "    'part_specs_ripplecurrent_ac__display_value', \n",
    "    'part_specs_impedance_display_value', \n",
    "    'part_specs_outsidediameter_display_value', \n",
    "    'part_specs_workingvoltage_display_value', \n",
    "    'part_specs_frequency_display_value'\n",
    "]\n",
    "\n",
    "string_to_float_cols.extend(year_specific_columns.get('float', []))\n",
    "string_to_int_cols.extend(year_specific_columns.get('int', []) + years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df43e5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shalinshah/dev/componet/processor/compute.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[cols] = df[cols].applymap(_convert_to_float, na_action=\"ignore\")\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:195: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[cols] = df[cols].applymap(_convert_to_int, na_action=\"ignore\")\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:173: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[cols] = df[cols].applymap(_convert_to_base_units, na_action=\"ignore\")\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:205: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"ceramic_class\"]] = df.loc[df[\"part_category_id\"] == category_id][\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:249: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"dielectric\"]] = df.apply(_get_dielectric, axis=1)\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:255: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"category\"]] = df[\"part_category_id\"]\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:261: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"manufacturer\"]] = df[\"part_manufacturer_name\"]\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:267: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"mpn\"]] = df[\"part_mpn\"]\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"voltage\"]] = (\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"current\"]] = df[\"part_specs_ripplecurrent_display_value\"].fillna(\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:285: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"capacitance\"]] = df.apply(_get_capacitance, axis=1)\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:322: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"esr\"]] = (\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:336: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"esr_frequency\"]] = df[\"part_specs_testfrequency_display_value\"]\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:339: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"esr_frequency_low\"]] = df[\"esr_frequency\"].apply(\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:342: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_map[\"esr_frequency_high\"]] = df[\"esr_frequency\"].apply(\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:352: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{column_map['price']}_{year}\"] = df[\n",
      "/Users/shalinshah/dev/componet/processor/compute.py:352: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{column_map['price']}_{year}\"] = df[\n"
     ]
    }
   ],
   "source": [
    "df = compute.spec_string_to_float(df, cols=string_to_float_cols)\n",
    "df = compute.spec_string_to_int(df, cols=string_to_int_cols)\n",
    "df = compute.spec_string_to_base_float(df, cols=string_to_base_float_cols)\n",
    "\n",
    "df = compute.classify_ceramic(df)\n",
    "df = compute.classify_dielectric(df)\n",
    "\n",
    "df = compute.process_category(df)\n",
    "df = compute.process_manufacturer(df)\n",
    "df = compute.process_mpn(df)\n",
    "df = compute.process_voltage(df)\n",
    "df = compute.process_current(df)\n",
    "df = compute.process_capacitance(df)\n",
    "df = compute.process_esr(df)\n",
    "df = compute.process_esr_frequency(df)\n",
    "df = compute.process_price(df, years)\n",
    "\n",
    "df = compute.compute_volume(df)\n",
    "df = compute.compute_mass(df)\n",
    "df = compute.compute_energy(df)\n",
    "df = compute.compute_rated_power(df)\n",
    "df = compute.compute_gravimetric_energy_density(df)\n",
    "df = compute.compute_volumetric_energy_density(df)\n",
    "df = compute.compute_gravimetric_power_density(df)\n",
    "df = compute.compute_volumetric_power_density(df)\n",
    "df = compute.compute_energy_per_cost(df, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824c723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [item for item in list(categories.column_map.values()) if item not in categories.columns_that_update_yearly_postprocess]\n",
    "cols_to_drop = cols_to_drop + years + year_specific_columns_postprocessed\n",
    "df = compute.drop_all_except(df, cols=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20dc99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{CSV_DIR}/{CSV_FINAL}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be67876",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ddc422",
   "metadata": {},
   "source": [
    "In this step, we want to take the final CSV that we generated, and actually upload it to our PostgreSQL database. We will use a package called (https://pypi.org/project/postgres-csv-uploader/) that was developed for this project in order to both infer the types for each of the columns in the CSV, create the necessary table schema, and then handle the upload process for the actual contents to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f43322f-340d-4a7a-84ed-137835853633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/Cellar/gpgme/1.23.2/lib/python3.12/site-packages/gpg-1.23.2-py3.12-macosx-13-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (23.3.2)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/Cellar/gpgme/1.23.2/lib/python3.12/site-packages/gpg-1.23.2-py3.12-macosx-13-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: postgres_csv_uploader in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/shalinshah/Library/Python/3.12/lib/python/site-packages (from postgres_csv_uploader) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.4.4 in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (from postgres_csv_uploader) (2.1.4)\n",
      "Requirement already satisfied: psycopg2>=2.7.7 in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (from postgres_csv_uploader) (2.9.9)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.41 in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (from postgres_csv_uploader) (2.0.23)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/opt/python-dateutil/lib/python3.12/site-packages (from pandas>=1.4.4->postgres_csv_uploader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (from pandas>=1.4.4->postgres_csv_uploader) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (from pandas>=1.4.4->postgres_csv_uploader) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (from SQLAlchemy>=1.4.41->postgres_csv_uploader) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/opt/six/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.4->postgres_csv_uploader) (1.16.0)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/Cellar/gpgme/1.23.2/lib/python3.12/site-packages/gpg-1.23.2-py3.12-macosx-13-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: psycopg2 in /opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/lib/python3.12/site-packages (2.9.9)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.12/site-packages/typing_extensions-4.9.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!/opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/bin/python -m pip install --upgrade pip\n",
    "!/opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/bin/python -m pip install postgres_csv_uploader\n",
    "!/opt/homebrew/Cellar/jupyterlab/4.0.9_2/libexec/bin/python -m pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a76aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postgres_csv_uploader.uploader import PostgresCSVUploader\n",
    "import psycopg2 as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"ec2-34-233-115-14.compute-1.amazonaws.com\"\n",
    "port = 5432\n",
    "database = \"dfu56m15dkhh46\"\n",
    "user = \"pgyrjmstmyerfk\"\n",
    "password = \"228fcbba14e9d2bf362fcaa29cabe1106cc8dba00605f45ee25e810194309fd4\"\n",
    "\n",
    "conn = ps.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "uploader = PostgresCSVUploader(conn)\n",
    "uploader.upload(\n",
    "    f'{CSV_DIR}/{CSV_FINAL}',\n",
    "    CSV_FINAL.split('.')[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532095d6",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67852cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JSON_2022 = \"./_raw_json/2022\"\n",
    "DATA_JSON_2023 = \"./_raw_json/2023\"\n",
    "\n",
    "DATA_CSV_2022 = \"./_raw_csv/2022\"\n",
    "DATA_CSV_2023 = \"./_raw_csv/2023\"\n",
    "\n",
    "DATA_CSV_COMBINED = \"./raw_csv/combined.csv\"\n",
    "DATA_CSV_FINAL = \"./raw_csv/final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb879ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_json(path: str):\n",
    "    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(os.path.join(path, json_file), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            results = data[\"data\"][\"search\"][\"results\"]\n",
    "            all_results = len(results)\n",
    "            unique_results = len(set([result[\"part\"][\"mpn\"] for result in results]))\n",
    "            print(f\"{json_file} unique elements: {unique_results}\")\n",
    "\n",
    "def print_unique_csv(path: str):\n",
    "    csv_files = [f for f in os.listdir(path) if f.endswith('.csv') and \"all\" not in f and \"combined\" not in f]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(path, csv_file))\n",
    "        print(f\"{csv_file} unique elements: {len(df['part_mpn'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b19a4",
   "metadata": {},
   "source": [
    "### Before and After Converting to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22c4fa-86d9-4e39-b982-635d90d3cd06",
   "metadata": {},
   "source": [
    "#### JSON [2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df10ba-091c-48ca-b7db-96f42ee087b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_json(DATA_JSON_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7055ec6-bbf2-4b2f-a25a-6d03cfadedcb",
   "metadata": {},
   "source": [
    "#### JSON [2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528046f7-1bdc-4387-aa0a-f75077e786c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_json(DATA_JSON_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a8fa3-3643-4efe-9c45-461ba1c9753c",
   "metadata": {},
   "source": [
    "#### CSV [2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c638cd7-9571-4bf0-aac5-3259db7323ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_csv(DATA_CSV_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4591b-cc1c-4438-9140-f2f578bfc8d8",
   "metadata": {},
   "source": [
    "#### CSV [2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363bfc6c-66df-42bd-be15-81503f7699f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_csv(DATA_CSV_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fa5b0",
   "metadata": {},
   "source": [
    "### Before and After Merging Categories & Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [d for d in os.listdir(CSV_DIR) if os.path.isdir(os.path.join(CSV_DIR, d))]\n",
    "\n",
    "unique_mpn_raw = set()\n",
    "for year in years:nmmn \n",
    "    year_dir = os.path.join(CSV_DIR, year)\n",
    "    for filename in os.listdir(year_dir):\n",
    "        if \"all\" not in filename and \"combined\" not in filename and filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(year_dir, filename))\n",
    "            unique_mpn_raw.update(df['part_mpn'].unique())\n",
    "\n",
    "print(f\"Total unique components across {years}: {len(unique_mpn_raw)}\")\n",
    "\n",
    "# Assuming combined_df is the dataframe after custom merging\n",
    "combined_df = pd.read_csv(DATA_CSV_COMBINED)\n",
    "unique_mpn_combined = combined_df['part_mpn'].nunique()\n",
    "print(f\"Total unique components in combined CSV: {unique_mpn_combined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc066d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f26239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f257eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1642f080",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c83074",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_viz = pd.read_csv(\"final.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699c99d",
   "metadata": {},
   "source": [
    "### Capacitance vs. Rated DC Voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"voltage\"\n",
    "y = \"capacitance\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Capacitance vs. Rated DC Voltage') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b4eae",
   "metadata": {},
   "source": [
    "### Rated Voltage vs. Volumetric Energy Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d73cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"voltage\"\n",
    "y = \"volumetric_energy_density\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Rated Voltage vs. Volumetric Energy Density') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ca09c",
   "metadata": {},
   "source": [
    "### Rated Voltage vs. Gravimetric Energy Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d24064",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"voltage\"\n",
    "y = \"gravimetric_energy_density\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Rated Voltage vs. Gravimetric Energy Density') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dee566",
   "metadata": {},
   "source": [
    "### Rated Voltage vs. Energy per Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"voltage\"\n",
    "y = \"energy_per_cost\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Rated Voltage vs. Energy per Cost') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d85f2b",
   "metadata": {},
   "source": [
    "### Volumetric Energy Density vs Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"volumetric_energy_density\"\n",
    "y = \"power\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Volumetric Energy Density vs Power') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d193b4",
   "metadata": {},
   "source": [
    "### Volumetric Energy Density vs Volumetric Power Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53926682",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"volumetric_energy_density\"\n",
    "y = \"volumetric_power_density\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Volumetric Energy Density vs Volumetric Power Density') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befecf9d",
   "metadata": {},
   "source": [
    "### Volumetric Energy Density vs Gravimetric Power Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce047fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"volumetric_energy_density\"\n",
    "y = \"gravimetric_power_density\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Volumetric Energy Density vs Gravimetric Power Density') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a67159-fafc-4315-acfc-1d5cd1d8bf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"capacitance\"\n",
    "y = \"mass\"\n",
    "one = data_viz.loc[data_viz[\"part_category_id\"] == 6331].plot.scatter(x=x, y=y, label=\"Aluminum Electrolytic Capacitors\", loglog=True, color=\"Blue\", s=5, title='Volumetric Energy Density vs Power') \n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6332].plot.scatter(x=x, y=y, label=\"Ceramic Capacitors\", loglog=True, ax=one, color=\"Green\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6333].plot.scatter(x=x, y=y, label=\"Film Capacitors\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6334].plot.scatter(x=x, y=y, label=\"Mica Capacitors\", loglog=True, ax=one, color=\"Orange\", s=5)\n",
    "data_viz.loc[data_viz[\"part_category_id\"] == 6336].plot.scatter(x=x, y=y, label=\"Tantalum Capacitors\", loglog=True, ax=one, color=\"Purple\", s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd382694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"mass\"\n",
    "y = \"capacitance\"\n",
    "one = data_viz.loc[pd.notna(data_viz[\"esr_frequency_low\"])].plot.scatter(x=x, y=y, label=\"Low Frequency\", loglog=True, color=\"Blue\", s=5, title='Volumetric Energy Density vs Power') \n",
    "data_viz.loc[pd.notna(data_viz[\"esr\"]) & pd.isna(data_viz[\"esr_frequency\"])].plot.scatter(x=x, y=y, label=\"Other\", loglog=True, ax=one, color=\"Red\", s=5)\n",
    "data_viz.loc[pd.notna(data_viz[\"esr_frequency_high\"])].plot.scatter(x=x, y=y, label=\"High Frequency\", loglog=True, ax=one, color=\"Green\", s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7113e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
